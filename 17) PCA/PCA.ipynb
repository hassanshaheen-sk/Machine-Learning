{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Image Classification on MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(18, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(0,10):\n",
    "    image = X[i]\n",
    "    image = np.array(image, dtype='int')\n",
    "    pixels = image.reshape((28, 28))\n",
    "    axs[i].imshow(pixels, cmap='gray')\n",
    "    axs[i].set_title('Label is '+str(Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_input(img, ax):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if img[x][y]<thresh else 'black')\n",
    "\n",
    "first_image = X[0]\n",
    "first_image = np.array(first_image, dtype='int')\n",
    "pixels = first_image.reshape((28, 28)) \n",
    "fig = plt.figure(figsize = (10,10)) \n",
    "ax = fig.add_subplot(111)\n",
    "visualize_input(pixels, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "print(xtrain.shape,ytrain.shape)\n",
    "print(xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(xtrain)\n",
    "\n",
    "\n",
    "train_img=pca.transform(xtrain)\n",
    "test_img=pca.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_img.shape,ytrain.shape)\n",
    "print(test_img.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Define instance of Logistic regression with solver as 'lbfgs', multi_class as 'multinomial', max_iter=300 and random state as 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=300, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Fit model on training images and training label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "model.fit(train_img, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Get Prediction on testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model.predict(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Calculate and print accuracy score for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy=accuracy_score(ytest, pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Print classification report  and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#write code here\n",
    "\n",
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Get weighted average of F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1= f1_score(ytest, pred, average = 'weighted')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can see that both Acuracy and F1 Score are around 91.4% and it is really good score for 154 components instead of all 784 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do comparison of Model performance against difference Maximal Variance and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining list of Variance\n",
    "pca_var=[1.0,.99,.95,.90,.85,.80,.75]\n",
    "\n",
    "#defining empty lists\n",
    "accuracy_list=[]\n",
    "f1_list=[]\n",
    "pca_components=[]\n",
    "training_time=[]\n",
    "prediction_time=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in pca_var:\n",
    "    \n",
    "    #applying PCA\n",
    "    if v==1.0:\n",
    "        train_img=xtrain\n",
    "        test_img=xtest\n",
    "        \n",
    "    else:\n",
    "        pca = PCA(v,random_state=42)\n",
    "        pca.fit(xtrain)\n",
    "        train_img=pca.transform(xtrain)\n",
    "        test_img=pca.transform(xtest)\n",
    "    \n",
    "    #model\n",
    "    model=LogisticRegression(solver='lbfgs',multi_class='multinomial', max_iter=300, random_state=42)\n",
    "    \n",
    "    #training\n",
    "    t_start = timeit.default_timer()\n",
    "    model.fit(train_img,ytrain)\n",
    "    t_stop = timeit.default_timer()\n",
    "    \n",
    "    #prediction\n",
    "    p_start = timeit.default_timer()\n",
    "    pred=model.predict(test_img)\n",
    "    p_stop = timeit.default_timer()\n",
    "    \n",
    "    #Append data in lists\n",
    "    accuracy_list.append(np.round(accuracy_score(ytest,pred),4))\n",
    "    f1_list.append(np.round(f1_score(ytest,pred, average='weighted'),4))\n",
    "    pca_components.append(train_img.shape[1])\n",
    "    training_time.append(np.round(t_stop-t_start,0))\n",
    "    prediction_time.append(np.round(p_stop-p_start,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating comparison data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict={'Variance Retained':pca_var,'Number of Components':pca_components,\n",
    "               'Training Time':training_time,'Prediction Time':prediction_time,\n",
    "               'Accuracy Score':accuracy_list,'F1 Score':f1_list}\n",
    "comparison=pd.DataFrame(comparison_dict)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that Model performance is not much degraded untill 87 components compared to all 784 components. Hence we can use 90% Variance using PCA to perform image classification in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting PCA and Model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it “serializes” the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/466/1*Menz4NWvM6Ca8H6B0d6VrQ.png' width=\"200\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting pickle file for PCA at 90% Explained Variance to be used while transforming data in Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.90,random_state=42)\n",
    "pca.fit(xtrain)\n",
    "train_img=pca.transform(xtrain)\n",
    "pk.dump(pca, open(\"pca.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting pickle file for Trained Model to be used for taking prediction of future images after Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(solver='lbfgs',multi_class='multinomial', max_iter=300, random_state=42)\n",
    "model.fit(train_img,ytrain)\n",
    "pk.dump(model, open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visalizing PCA Components and Image Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Define empty PCA instance with randome state 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if n_components is not set all components are kept (784 in this case)\n",
    "pca = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Fit PCA on X (without scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Check number of components using .n_components_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code here\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating total varaince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(pca.explained_variance_)\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Percentage of Explained Variance against each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_, reverse=True)] \n",
    "print(var_exp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Cumilative Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot can help you understand the level of redundancy/information present in multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.step(range(1, 785), cum_var_exp,label='cumulative explained variance')\n",
    "plt.title('Cumulative Explained Variance as a Function of the Number of Components')\n",
    "plt.ylabel('Cumulative Explained variance')\n",
    "plt.xlabel('Principal components')\n",
    "plt.axhline(y = 95, color='k', linestyle='--', label = '95% Explained Variance')\n",
    "plt.axhline(y = 90, color='c', linestyle='--', label = '90% Explained Variance')\n",
    "plt.axhline(y = 85, color='r', linestyle='--', label = '85% Explained Variance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plotting Image Compression against 100%, 99%, 95%, 90%, and 85% of Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainedVariance(percentage, images): \n",
    "    pca = PCA(percentage)\n",
    "    pca.fit(images)\n",
    "    components = pca.transform(images)\n",
    "    approxOriginal = pca.inverse_transform(components)\n",
    "    return approxOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_var=[1.0,.99,.95,.90,.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [784, np.argmax(cum_var_exp > 99) + 1, np.argmax(cum_var_exp > 95) + 1, np.argmax(cum_var_exp > 90) + 1, \n",
    "              np.argmax(cum_var_exp >= 85) + 1]\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,5, figsize=(18, 8))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i,perc in enumerate(pca_var):\n",
    "    if perc==1.0:\n",
    "        axs[i].imshow(X[17].reshape(28,28),cmap='gray');\n",
    "        axs[i].set_xlabel(str(components[i])+' Components', fontsize = 12)\n",
    "        axs[i].set_title('Original Image', fontsize = 14)\n",
    "    else:\n",
    "        axs[i].imshow(explainedVariance(perc,X)[17].reshape(28,28),cmap='gray');\n",
    "        axs[i].set_xlabel(str(components[i])+' Components', fontsize = 12)\n",
    "        axs[i].set_title(str(int(perc*100))+'% of Explained Variance', fontsize = 14)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
